package cs.Lab2.WordCount;

import java.io.IOException;
import java.util.Iterator;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;


public class FirstTaskReducer extends Reducer< Text, IntWritable, Text, IntWritable> {

    // Overriding of the reduce function

    @Override
    protected void reduce(Text key, Iterable <IntWritable> values, Context context) throws IOException,InterruptedException{
        int NbWordDoc = 0;

    	Iterator<IntWritable> i = values.iterator();
    	
    	int count = 0;
    	
    	while(i.hasNext())
    	{
    		count += i.next().get();
    	}
    	NbWordDoc += count;
		context.write(key, new IntWritable(count)); // Classique WordCount, but we split the word from the different docs

    }

}


